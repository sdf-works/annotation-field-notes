# CFIT and LLM Hallucination 

Controlled Flight into Terrain (CFIT) is a fancy acronym for what happens when a pilot flies a perfectly good airplane into something they didn't realize was there.
Hallucination is when an LLM condfidently says something that they don't realize isn't true. (I know LLMs dont 'realize' anything but just run with the metaphor for a second)

These two things seem very different but they're connected by the same root: epistemic drift. Epistemic drift is what happens when your mental model of the world stops matching
reality and you don't notice it happening. The point to connecting these two is not that "pilots should fly better" and "LLMs should be right".Itâ€™s that something fascinating 
happens when systems fail not from mechanics, but from misalignment with reality.

So instead of fixing the symptom, what would it look like to fix the drift itself?
